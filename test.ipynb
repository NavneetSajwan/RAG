{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navneet/Documents/AIML/RAG/rag/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import  ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    model = \"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant. Your name is Friday\"),\n",
    "    HumanMessage(content= \"Hi Friday, how are you today?\"),\n",
    "    SystemMessage(content=\"Hi, I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I would like to understand God particle.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The \"God particle\" is a term used to refer to the Higgs boson, a particle that was discovered in 2012 at the Large Hadron Collider (LHC) in Geneva, Switzerland. The Higgs boson is a fundamental particle in the Standard Model of particle physics, which is a theory that describes the fundamental particles and forces in the universe.\\n\\nThe Higgs boson is particularly interesting because it is associated with the Higgs field, which is believed to give particles their mass. According to the Standard Model, all particles acquire mass by interacting with this field. The Higgs boson is the particle associated with the Higgs field, and its discovery confirmed the existence of this field.\\n\\nThe particle is nicknamed the \"God particle\" due to the book title \"The God Particle: If the Universe is the Answer, What is the Question?\" by physicist Leon Lederman. The nickname was chosen for marketing purposes, but many scientists do not use it because it can be misleading and create confusion.\\n\\nThe discovery of the Higgs boson was a significant achievement in the field of particle physics, as it confirmed a key component of the Standard Model. It helps us understand why particles have mass and how they interact with the Higgs field. This knowledge is crucial for our understanding of the fundamental building blocks of the universe.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navneet/Documents/AIML/RAG/rag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key=os.environ.get(\"PINCONE_API_KEY\"),\n",
    "    environment=\"gcp-starter\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = 'llama-2-rag'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine'\n",
    "    )\n",
    "\n",
    "while not pinecone.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model  = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-ada-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"sun rises in the east\", \"gravity is not a force\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = embed_model.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 409/409 [00:00<00:00, 608kB/s]\n",
      "Downloading data: 100%|██████████| 14.4M/14.4M [00:06<00:00, 2.12MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:06<00:00,  6.84s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 424.65it/s]\n",
      "Generating train split: 4838 examples [00:00, 47941.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': Value(dtype='string', id=None),\n",
       " 'chunk-id': Value(dtype='string', id=None),\n",
       " 'chunk': Value(dtype='string', id=None),\n",
       " 'id': Value(dtype='string', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'summary': Value(dtype='string', id=None),\n",
       " 'source': Value(dtype='string', id=None),\n",
       " 'authors': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'categories': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'comment': Value(dtype='string', id=None),\n",
       " 'journal_ref': Value(dtype='string', id=None),\n",
       " 'primary_category': Value(dtype='string', id=None),\n",
       " 'published': Value(dtype='string', id=None),\n",
       " 'updated': Value(dtype='string', id=None),\n",
       " 'references': [{'id': Value(dtype='string', id=None),\n",
       "   'title': Value(dtype='string', id=None),\n",
       "   'authors': Value(dtype='string', id=None),\n",
       "   'year': Value(dtype='string', id=None)}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4838"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>0</td>\n",
       "      <td>High-Performance Neural Networks\\nfor Visual O...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>1</td>\n",
       "      <td>January 2011\\nAbstract\\nWe present a fast, ful...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>2</td>\n",
       "      <td>promising architectures for such tasks. The mo...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>3</td>\n",
       "      <td>Mutch and Lowe, 2008), whose \flters are \fxed, ...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>4</td>\n",
       "      <td>We evaluate various networks on the handwritte...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doi chunk-id                                              chunk  \\\n",
       "0  1102.0183        0  High-Performance Neural Networks\\nfor Visual O...   \n",
       "1  1102.0183        1  January 2011\\nAbstract\\nWe present a fast, ful...   \n",
       "2  1102.0183        2  promising architectures for such tasks. The mo...   \n",
       "3  1102.0183        3  Mutch and Lowe, 2008), whose \n",
       "lters are \n",
       "xed, ...   \n",
       "4  1102.0183        4  We evaluate various networks on the handwritte...   \n",
       "\n",
       "          id                                              title  \\\n",
       "0  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "1  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "2  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "3  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "4  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We present a fast, fully parameterizable GPU i...   \n",
       "1  We present a fast, fully parameterizable GPU i...   \n",
       "2  We present a fast, fully parameterizable GPU i...   \n",
       "3  We present a fast, fully parameterizable GPU i...   \n",
       "4  We present a fast, fully parameterizable GPU i...   \n",
       "\n",
       "                           source  \\\n",
       "0  http://arxiv.org/pdf/1102.0183   \n",
       "1  http://arxiv.org/pdf/1102.0183   \n",
       "2  http://arxiv.org/pdf/1102.0183   \n",
       "3  http://arxiv.org/pdf/1102.0183   \n",
       "4  http://arxiv.org/pdf/1102.0183   \n",
       "\n",
       "                                             authors      categories  \\\n",
       "0  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "1  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "2  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "3  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "4  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "\n",
       "                         comment journal_ref primary_category published  \\\n",
       "0  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "1  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "2  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "3  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "4  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "\n",
       "    updated references  \n",
       "0  20110201         []  \n",
       "1  20110201         []  \n",
       "2  20110201         []  \n",
       "3  20110201         []  \n",
       "4  20110201         []  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['promising architectures for such tasks. The most successful hierarchical object recognition systems',\n",
       " 'all extract localized features from input images, convolving image patches with \\x0clters. Filter',\n",
       " 'responses are then repeatedly sub-sampled and re-\\x0cltered, resulting in a deep feed-forward network',\n",
       " 'architecture whose output feature vectors are eventually classi\\x0ced. One of the \\x0crst hierarchical',\n",
       " 'neural systems was the Neocognitron (Fukushima, 1980) which inspired many of the more recent',\n",
       " 'variants.',\n",
       " 'Unsupervised learning methods applied to patches of natural images tend to produce localized',\n",
       " '\\x0clters that resemble o\\x0b-center-on-surround \\x0clters, orientation-sensitive bar detectors, Gabor \\x0clters',\n",
       " '(Schmidhuber et al. , 1996; Olshausen and Field, 1997; Hoyer and Hyv\\x7f arinen, 2000). These \\x0cndings',\n",
       " 'in conjunction with experimental studies of the visual cortex justify the use of such \\x0clters in the',\n",
       " 'so-called standard model for object recognition (Riesenhuber and Poggio, 1999; Serre et al. , 2007;',\n",
       " 'Mutch and Lowe, 2008), whose \\x0clters are \\x0cxed, in contrast to those of Convolutional Neural']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.chunk[2].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby(\"id\").get_group('1102.0183')# each row contains a  some amount of text from a paper. The paper can be keyed using the id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>0</td>\n",
       "      <td>High-Performance Neural Networks\\nfor Visual O...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>1</td>\n",
       "      <td>January 2011\\nAbstract\\nWe present a fast, ful...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>2</td>\n",
       "      <td>promising architectures for such tasks. The mo...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>3</td>\n",
       "      <td>Mutch and Lowe, 2008), whose \flters are \fxed, ...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>4</td>\n",
       "      <td>We evaluate various networks on the handwritte...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doi chunk-id                                              chunk  \\\n",
       "0  1102.0183        0  High-Performance Neural Networks\\nfor Visual O...   \n",
       "1  1102.0183        1  January 2011\\nAbstract\\nWe present a fast, ful...   \n",
       "2  1102.0183        2  promising architectures for such tasks. The mo...   \n",
       "3  1102.0183        3  Mutch and Lowe, 2008), whose \n",
       "lters are \n",
       "xed, ...   \n",
       "4  1102.0183        4  We evaluate various networks on the handwritte...   \n",
       "\n",
       "          id                                              title  \\\n",
       "0  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "1  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "2  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "3  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "4  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We present a fast, fully parameterizable GPU i...   \n",
       "1  We present a fast, fully parameterizable GPU i...   \n",
       "2  We present a fast, fully parameterizable GPU i...   \n",
       "3  We present a fast, fully parameterizable GPU i...   \n",
       "4  We present a fast, fully parameterizable GPU i...   \n",
       "\n",
       "                           source  \\\n",
       "0  http://arxiv.org/pdf/1102.0183   \n",
       "1  http://arxiv.org/pdf/1102.0183   \n",
       "2  http://arxiv.org/pdf/1102.0183   \n",
       "3  http://arxiv.org/pdf/1102.0183   \n",
       "4  http://arxiv.org/pdf/1102.0183   \n",
       "\n",
       "                                             authors      categories  \\\n",
       "0  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "1  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "2  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "3  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "4  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "\n",
       "                         comment journal_ref primary_category published  \\\n",
       "0  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "1  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "2  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "3  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "4  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "\n",
       "    updated references  \n",
       "0  20110201         []  \n",
       "1  20110201         []  \n",
       "2  20110201         []  \n",
       "3  20110201         []  \n",
       "4  20110201         []  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1090, 1308, 1077, 1246)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find the length of the each chunk\n",
    "len(df1.chunk[0]), len(df1.chunk[1]), len(df1.chunk[2]), len(df1.chunk[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>0</td>\n",
       "      <td>High-Performance Neural Networks\\nfor Visual O...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>1</td>\n",
       "      <td>January 2011\\nAbstract\\nWe present a fast, ful...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>2</td>\n",
       "      <td>promising architectures for such tasks. The mo...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doi chunk-id                                              chunk  \\\n",
       "0  1102.0183        0  High-Performance Neural Networks\\nfor Visual O...   \n",
       "1  1102.0183        1  January 2011\\nAbstract\\nWe present a fast, ful...   \n",
       "2  1102.0183        2  promising architectures for such tasks. The mo...   \n",
       "\n",
       "          id                                              title  \\\n",
       "0  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "1  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "2  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We present a fast, fully parameterizable GPU i...   \n",
       "1  We present a fast, fully parameterizable GPU i...   \n",
       "2  We present a fast, fully parameterizable GPU i...   \n",
       "\n",
       "                           source  \\\n",
       "0  http://arxiv.org/pdf/1102.0183   \n",
       "1  http://arxiv.org/pdf/1102.0183   \n",
       "2  http://arxiv.org/pdf/1102.0183   \n",
       "\n",
       "                                             authors      categories  \\\n",
       "0  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "1  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "2  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "\n",
       "                         comment journal_ref primary_category published  \\\n",
       "0  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "1  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "2  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "\n",
       "    updated references  \n",
       "0  20110201         []  \n",
       "1  20110201         []  \n",
       "2  20110201         []  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [04:32<00:00,  5.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navneet/Documents/AIML/RAG/rag/lib/python3.9/site-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text_field=\"text\"\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query,\n",
    "    text_field\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is special about llama2?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='models will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augment_prompt = f\"\"\" Using the context below, answer the query.\n",
    "\n",
    "context: {source_knowledge}\n",
    "\n",
    "query: {query}\n",
    "\"\"\"\n",
    "    return augment_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Using the context below, answer the query.\\n\\ncontext: Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\\n\\nquery: What is special about llama2?\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_prompt(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(content=augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant. Your name is Friday', additional_kwargs={}),\n",
       " HumanMessage(content='Hi Friday, how are you today?', additional_kwargs={}, example=False),\n",
       " SystemMessage(content=\"Hi, I'm great thank you. How can I help you?\", additional_kwargs={}),\n",
       " HumanMessage(content='I would like to understand God particle.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=' Using the context below, answer the query.\\n\\ncontext: Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\\n\\nquery: What is special about llama2?\\n', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The passage states that Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging from 7 billion to 70 billion parameters. These models, specifically L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases and outperform open-source chat models on most benchmarks. They are intended for commercial and research use in English and can be adapted for various natural language generation tasks. Llama 2 models also undergo fine-tuning and safety measures to align with human preferences and enhance usability and safety.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_me(query):\n",
    "    prompt = augment_prompt(query)\n",
    "    messages.append(HumanMessage(content=prompt))\n",
    "    return chat(messages).content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, specifically L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. They have been shown to outperform open-source chat models on most benchmarks and may serve as a suitable substitute for closed-source models. The closed-source LLMs, such as ChatGPT, BARD, and Claude, have been heavily fine-tuned to align with human preferences, enhancing their usability and safety. Llama 2 models are intended for commercial and research use in English, with tuned models suitable for assistant-like chat and pretrained models adaptable to various natural language generation tasks.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_me(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_me():\n",
    "    while(True):\n",
    "        query = input()\n",
    "        prompt = augment_prompt(query)\n",
    "        messages.append(HumanMessage(content=prompt))\n",
    "        res = chat(messages)\n",
    "        print(res.content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant. Your name is Friday', additional_kwargs={}),\n",
       " HumanMessage(content='Hi Friday, how are you today?', additional_kwargs={}, example=False),\n",
       " SystemMessage(content=\"Hi, I'm great thank you. How can I help you?\", additional_kwargs={}),\n",
       " HumanMessage(content='I would like to understand God particle.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=' Using the context below, answer the query.\\n\\ncontext: Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\\n\\nquery: What is special about llama2?\\n', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=' Using the context below, answer the query.\\n\\ncontext: Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\\n\\nquery: What is special about llama2?\\n', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=' Using the context below, answer the query.\\n\\ncontext: Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\\n\\nquery: tell me briefly what is llama2\\n', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=' Using the context below, answer the query.\\n\\ncontext: u\\nu0\\nwrkqPfKX91+zNKIK2SSGtPx3AT9jGoUTPJpqZsanlA2ogPesVRRu8bP5vdOyZlV+iSMtS2FZK7+nshoZMwkCmxnRHFolr2Z+J/XSTG89jOhkhS5YotFYSoJxmT2POkLzRnKiSWUaWFvJWxINWVoIyrZELzll1dJ86LquVXv4bJSu8njKMIJnMI5eHAFNbiDOjSAgYRneIU3Z+y8OO/Ox6K14OQzx/AHzucPqJqPrw==</latexit>Gout\\n\\nquery: great\\n', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=' Using the context below, answer the query.\\n\\ncontext: whichmodels areprompted toexplain theirreasoningwhen givena complexproblem, inorder toincrease\\nthe likelihood that their ﬁnal answer is correct.\\nRLHF has emerged as a powerful strategy for ﬁne-tuning Large Language Models, enabling signiﬁcant\\nimprovements in their performance (Christiano et al., 2017). The method, ﬁrst showcased by Stiennon et al.\\n(2020) in the context of text-summarization tasks, has since been extended to a range of other applications.\\nIn this paradigm, models are ﬁne-tuned based on feedback from human users, thus iteratively aligning the\\nmodels’ responses more closely with human expectations and preferences.\\nOuyang et al. (2022) demonstrates that a combination of instruction ﬁne-tuning and RLHF can help ﬁx\\nissues with factuality, toxicity, and helpfulness that cannot be remedied by simply scaling up LLMs. Bai\\net al. (2022b) partially automates this ﬁne-tuning-plus-RLHF approach by replacing the human-labeled\\nﬁne-tuningdatawiththemodel’sownself-critiquesandrevisions,andbyreplacinghumanraterswitha\\nandAlpaca(Taorietal.,2023)adoptingauniqueapproachtotrainingwithsyntheticinstructions(Honovich\\net al., 2022; Wang et al., 2022). However, while these models show promise, they still fall short of the bar set\\nby their closed-source counterparts.\\nInstructionTuning. Weietal.(2021)obtainedzero-shotperformanceonunseentasksbyﬁne-tuningLLMs\\nonnumerousdatasets. Chungetal.(2022)andLongpreetal.(2023)investigatetheimpactofinstruction\\ntuningasafunctionofnumberoftasks,modelsize,promptsettings,etc. Promptsusedforinstructiontuning\\ncanbecreatedbyhumansorbyLLMsthemselves(Zhouetal.,2022),andfollow-upinstructionscanbeused\\ntoreﬁneinitialgenerationstomakethemmoreuseful,engaging,andunbiased(Gangulietal.,2023;Madaan\\net al., 2023). An approach related to instruction tuning is chain-of-thought prompting (Wei et al., 2022b), in\\nwhichmodels areprompted toexplain theirreasoningwhen givena complexproblem, inorder toincrease\\nthe likelihood that their ﬁnal answer is correct.\\nﬁne-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then\\nﬁne-tuned on very narrow task distributions. For instance [ HLW+20] observe that larger models do not necessarily\\ngeneralize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm\\ncan be poor because the model is overly speciﬁc to the training distribution and does not generalize well outside it\\n[YdC+19,MPL19 ]. Thus, the performance of ﬁne-tuned models on speciﬁc benchmarks, even when it is nominally at\\nhuman-level, may exaggerate actual performance on the underlying task [GSL+18, NK19].\\nThird, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural\\nlanguage (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number\\nof demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often\\nFigure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad\\n\\nquery: how do i fine tune it?\\n', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=' Using the context below, answer the query.\\n\\ncontext: data synthesis to further augment the data during training.\\nTraining on large quantities of data usually requires the use of larger models. Indeed, our models\\nhave many more parameters than those used in our previous system. Training a single model at\\nthese scales requires tens of exaFLOPs1that would require 3-6 weeks to execute on a single GPU.\\nThis makes model exploration a very time consuming exercise, so we have built a highly optimized\\ntraining system that uses 8 or 16 GPUs to train one model. In contrast to previous large-scale training\\napproaches that use parameter servers and asynchronous updates [18, 10], we use synchronous SGD,\\nwhich is easier to debug while testing new ideas, and also converges faster for the same degree of\\ndata parallelism. To make the entire system efﬁcient, we describe optimizations for a single GPU\\nas well as improvements to scalability for multiple GPUs. We employ optimization techniques\\ntypically found in High Performance Computing to improve scalability. These optimizations include\\na fast implementation of the CTC loss function on the GPU, and a custom memory allocator. We\\nalso use carefully integrated compute nodes and a custom implementation of all-reduce to accelerate\\ninter-GPU communication. Overall the system sustains approximately 50 teraFLOP/second when\\ntraining on 16 GPUs. This amounts to 3 teraFLOP/second per GPU which is about 50% of peak\\narchitectural constraints and sometimes domain speci\\x0cc knowledge about the problem, one can\\nalleviate the optimization di\\x0eculty that generic neural network architectures face.\\nIt could be that the combination of the network architecture and training procedure produces a training dynamics that tends to yield into these minima that are poor from the point of\\nview of generalization error, even when they manage to nail training error by providing enough\\ncapacity. Of course, as the number of examples increases, we would expect this discrepancy to\\ndecrease, but then the optimization problem could still make the task unfeasible in practice.\\nNote however that our preliminary experiments with increasing the training set size (8-fold) for\\nMLPs did not reveal signs of potential improvements in test error yet, as shown in Figure 14.\\nEven using online training on 545400 Pentomino examples, the SMLP-nohints architecture was\\nstill doing far from perfect in terms of generalization error (Figure 12).\\nThese \\x0cndings bring supporting evidence to the \\\\Guided Learning Hypothesis\" and \\\\Deeper\\nHarder Hypothesis\" from Bengio (2013a): higher level abstractions, which are expressed by\\ncomposing simpler concepts, are more di\\x0ecult to learn (with the learner often getting in an\\ne\\x0bective local minimum ), but that di\\x0eculty can be overcome if another agent provides hints\\nmemory (Hochreiter & Schmidhuber, 1997; Le et al., 2015;\\nGraves et al., 2014; Martens & Sutskever, 2011)\\nOf the handful of optimization algorithms we tried on the\\nvarious models, RMSProp (Tieleman & Hinton, 2012) lead\\nto fastest convergence and is what we stuck to for all experiments here on in. However, we found the IRNN to be\\nparticularly unstable; it only ran without blowing up with\\nincredibly low learning rates and gradient clipping. Since\\nthe performance was so poor relative to other models we\\ncompare against, we do not show IRNN curves in the ﬁgures. In each experiment we use a learning rate of 10\\x003\\nand a decay rate of 0:9. For the LSTM and RNN models,\\nwe had to clip gradients at 1 to avoid exploding gradients.Gradient clipping was unnecessary for the uRNN.\\n5.1. Copying memory problem\\nRecurrent networks have been known to have trouble remembering information about inputs seen many time steps\\npreviously (Bengio et al., 1994; Pascanu et al., 2010). We\\ntherefore want to test the uRNN’s ability to recall exactly\\ndata seen a long time ago.\\nFollowing a similar setup to (Hochreiter & Schmidhuber,\\n\\nquery: what memory optimizations can i make during training?\\n', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
